import requests
from bs4 import BeautifulSoup
import json
import time
from urllib.parse import urljoin
import csv

class ChowagokenScraper:
    def __init__(self):
        self.base_url = "https://www.chowagiken.co.jp/"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
    def get_page_content(self, url):
        """æŒ‡å®šã•ã‚ŒãŸURLã‹ã‚‰ãƒšãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—"""
        try:
            response = self.session.get(url)
            response.raise_for_status()
            response.encoding = 'utf-8'
            return response.text
        except requests.RequestException as e:
            print(f"ã‚¨ãƒ©ãƒ¼: {url} ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ - {e}")
            return None
    
    def scrape_main_page(self):
        """ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‹ã‚‰åŸºæœ¬æƒ…å ±ã‚’æŠ½å‡º"""
        html_content = self.get_page_content(self.base_url)
        if not html_content:
            return None
            
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # åŸºæœ¬æƒ…å ±ã‚’æŠ½å‡º
        company_info = {
            'company_name': 'æ ªå¼ä¼šç¤¾èª¿å’ŒæŠ€ç ”',
            'main_title': '',
            'main_description': '',
            'services': [],
            'features': [],
            'clients': [],
            'images': []
        }
        
        # ãƒ¡ã‚¤ãƒ³ã‚¿ã‚¤ãƒˆãƒ«ã¨èª¬æ˜æ–‡ã‚’æŠ½å‡º
        main_text = soup.get_text()
        lines = [line.strip() for line in main_text.split('\n') if line.strip()]
        
        # ã‚¿ã‚¤ãƒˆãƒ«éƒ¨åˆ†ã‚’æŠ½å‡º
        for i, line in enumerate(lines):
            if 'DXå®Ÿç¾' in line or 'AIæ´»ç”¨' in line:
                company_info['main_title'] = line
                if i + 1 < len(lines):
                    company_info['main_description'] = lines[i + 1]
                break
        
        # ã‚µãƒ¼ãƒ“ã‚¹æƒ…å ±ã‚’æŠ½å‡º
        service_keywords = ['ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°', 'AIé–‹ç™º', 'AIäººæè‚²æˆ']
        for line in lines:
            for keyword in service_keywords:
                if keyword in line and len(line) > 10:
                    company_info['services'].append(line)
                    break
        
        # ç‰¹å¾´ã‚’æŠ½å‡º
        feature_keywords = ['150ä»¶ä»¥ä¸Š', 'å­¦è¡“ãƒ¬ãƒ™ãƒ«', 'ç‹¬è‡ªã®é«˜å“è³ªã‚¨ãƒ³ã‚¸ãƒ³']
        for line in lines:
            for keyword in feature_keywords:
                if keyword in line and len(line) > 15:
                    company_info['features'].append(line)
                    break
        
        # ç”»åƒURLã‚’æŠ½å‡º
        images = soup.find_all('img')
        for img in images:
            src = img.get('src')
            if src:
                full_url = urljoin(self.base_url, src)
                alt_text = img.get('alt', '')
                company_info['images'].append({
                    'url': full_url,
                    'alt': alt_text
                })
        
        # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæƒ…å ±ã‚’æŠ½å‡ºï¼ˆç”»åƒã®altå±æ€§ã‹ã‚‰ï¼‰
        client_images = [img for img in images if 'logo' in img.get('src', '') or 'æ§˜' in img.get('alt', '')]
        for img in client_images:
            alt_text = img.get('alt', '')
            if alt_text and 'æ§˜' in alt_text:
                company_info['clients'].append(alt_text)
        
        return company_info
    
    def extract_contact_info(self, soup):
        """é€£çµ¡å…ˆæƒ…å ±ã‚’æŠ½å‡º"""
        contact_info = {}
        
        # ãŠå•ã„åˆã‚ã›ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¢ã™
        contact_text = soup.get_text()
        if 'ãŠå•ã„åˆã‚ã›' in contact_text:
            contact_info['contact_available'] = True
            contact_info['contact_message'] = 'ãŠæ°—è»½ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„'
        
        return contact_info
    
    def save_to_json(self, data, filename='chowagiken_data.json'):
        """ãƒ‡ãƒ¼ã‚¿ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"ãƒ‡ãƒ¼ã‚¿ãŒ {filename} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
        except Exception as e:
            print(f"JSONä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def save_to_csv(self, data, filename='chowagiken_data.csv'):
        """åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        try:
            with open(filename, 'w', encoding='utf-8', newline='') as f:
                writer = csv.writer(f)
                writer.writerow(['é …ç›®', 'å†…å®¹'])
                writer.writerow(['ä¼šç¤¾å', data.get('company_name', '')])
                writer.writerow(['ãƒ¡ã‚¤ãƒ³ã‚¿ã‚¤ãƒˆãƒ«', data.get('main_title', '')])
                writer.writerow(['èª¬æ˜', data.get('main_description', '')])
                
                # ã‚µãƒ¼ãƒ“ã‚¹
                for i, service in enumerate(data.get('services', [])):
                    writer.writerow([f'ã‚µãƒ¼ãƒ“ã‚¹{i+1}', service])
                
                # ç‰¹å¾´
                for i, feature in enumerate(data.get('features', [])):
                    writer.writerow([f'ç‰¹å¾´{i+1}', feature])
                
                # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
                for i, client in enumerate(data.get('clients', [])):
                    writer.writerow([f'ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ{i+1}', client])
            
            print(f"ãƒ‡ãƒ¼ã‚¿ãŒ {filename} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
        except Exception as e:
            print(f"CSVä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def scrape_all(self):
        """å…¨ã¦ã®æƒ…å ±ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        print("èª¿å’ŒæŠ€ç ”ã®ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­...")
        
        # ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        main_data = self.scrape_main_page()
        
        if main_data:
            print("\n=== å–å¾—ãƒ‡ãƒ¼ã‚¿ ===")
            print(f"ä¼šç¤¾å: {main_data['company_name']}")
            print(f"ãƒ¡ã‚¤ãƒ³ã‚¿ã‚¤ãƒˆãƒ«: {main_data['main_title']}")
            print(f"èª¬æ˜: {main_data['main_description']}")
            
            print(f"\nã‚µãƒ¼ãƒ“ã‚¹æ•°: {len(main_data['services'])}")
            for i, service in enumerate(main_data['services'], 1):
                print(f"  {i}. {service}")
            
            print(f"\nç‰¹å¾´æ•°: {len(main_data['features'])}")
            for i, feature in enumerate(main_data['features'], 1):
                print(f"  {i}. {feature}")
            
            print(f"\nã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ•°: {len(main_data['clients'])}")
            for i, client in enumerate(main_data['clients'], 1):
                print(f"  {i}. {client}")
            
            print(f"\nç”»åƒæ•°: {len(main_data['images'])}")
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
            self.save_to_json(main_data)
            self.save_to_csv(main_data)
            
            return main_data
        else:
            print("ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return None

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    scraper = ChowagokenScraper()
    
    try:
        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
        data = scraper.scrape_all()
        
        if data:
            print("\nâœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†!")
            print("ğŸ“ 'chowagiken_data.json' - è©³ç´°ãªJSONå½¢å¼")
            print("ğŸ“Š 'chowagiken_data.csv' - è¡¨å½¢å¼ã®ã‚µãƒãƒªãƒ¼")
        else:
            print("âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
    except KeyboardInterrupt:
        print("\nâš ï¸  ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

if __name__ == "__main__":
    main()